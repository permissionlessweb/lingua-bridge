# LinguaBridge Akash Network Deployment
#
# Three-tier architecture: text inference (GPU), voice inference (GPU), bot (CPU).
#
# Before deploying:
# 1. Generate admin keys: cargo run -p admin-cli -- keygen
# 2. Replace <YOUR_ADMIN_PUBLIC_KEY> with contents of admin.pub
# 3. Build and push Docker images to GHCR with explicit version tags
# 4. Update GHCR credentials below (username + PAT with read:packages scope)
# 5. Update image tags to match your pushed versions
#
# After deployment:
# 1. Note the assigned URIs from Akash
# 2. Provision the bot:
#    linguabridge-admin provision --bot-url https://<akash-uri>:9999 \
#      --discord-token "token" --admin-key admin.key

version: "2.0"

services:
  # TranslateGemma inference service (GPU required)
  # Base: nvidia/cuda:12.4.1-runtime-ubuntu22.04, Python 3.11
  inference:
    image: ghcr.io/permissionlessweb/linguabridge-inference:v0.1.0
    credentials:
      host: ghcr.io
      username: <GHCR_USERNAME>
      password: <GHCR_PAT>
    env:
      - "DEVICE=cuda"
      - "TORCH_DTYPE=bfloat16"
      - "TRANSLATEGEMMA_MODEL=google/translategemma-4b-it"
      - "HOST=0.0.0.0"
      - "PORT=8000"
      - "PYTHONUNBUFFERED=1"
      - "TRANSFORMERS_CACHE=/root/.cache/huggingface"
      - "HF_HOME=/root/.cache/huggingface"
    expose:
      - port: 8000
        as: 8000
        to:
          - service: bot

  # Voice inference service (STT/TTS/WebSocket) - GPU required
  # Base: nvidia/cuda:12.4.1-runtime-ubuntu22.04, Python 3.11
  # Models: Distil-Whisper (STT), CosyVoice (TTS), TranslateGemma (translation)
  voice-inference:
    image: ghcr.io/permissionlessweb/linguabridge-voice:v0.1.0
    credentials:
      host: ghcr.io
      username: <GHCR_USERNAME>
      password: <GHCR_PAT>
    env:
      - "DEVICE=cuda"
      - "TORCH_DTYPE=bfloat16"
      - "STT_MODEL=distil-large-v3"
      - "TTS_MODEL=CosyVoice2-0.5B"
      - "ENABLE_TTS=true"
      - "ENABLE_DIARIZATION=false"
      - "TRANSLATEGEMMA_MODEL=google/translategemma-4b-it"
      - "HF_TOKEN=<HF_TOKEN>"
      - "HOST=0.0.0.0"
      - "PORT=8001"
      - "PYTHONUNBUFFERED=1"
      - "TRANSFORMERS_CACHE=/root/.cache/huggingface"
      - "HF_HOME=/root/.cache/huggingface"
    expose:
      - port: 8001
        as: 8001
        to:
          - service: bot

  # LinguaBridge Discord bot and web server (CPU only)
  # Base: debian:trixie-slim, Rust binary
  bot:
    image: ghcr.io/permissionlessweb/linguabridge-bot:v0.1.0
    credentials:
      host: ghcr.io
      username: <GHCR_USERNAME>
      password: <GHCR_PAT>
    dependencies:
      - service: inference
      - service: voice-inference
    env:
      # Admin provisioning (public key only - safe to expose)
      - "LINGUABRIDGE_ADMIN__PUBLIC_KEY=<YOUR_ADMIN_PUBLIC_KEY>"
      - "LINGUABRIDGE_ADMIN__PORT=9999"
      - "LINGUABRIDGE_ADMIN__HOST=0.0.0.0"
      # Internal service URLs (Akash DNS resolution)
      - "LINGUABRIDGE_INFERENCE__URL=http://inference:8000"
      - "LINGUABRIDGE_VOICE__URL=ws://voice-inference:8001/voice"
      - "LINGUABRIDGE_VOICE__ENABLE_TTS_PLAYBACK=false"
      - "LINGUABRIDGE_VOICE__DEFAULT_TARGET_LANGUAGE=en"
      # Web server
      - "LINGUABRIDGE_WEB__HOST=0.0.0.0"
      - "LINGUABRIDGE_WEB__PORT=3000"
      - "LINGUABRIDGE_WEB__PUBLIC_URL=https://your-deployment.akash.network"
      # Database (persistent storage)
      - "LINGUABRIDGE_DATABASE__URL=sqlite:///data/linguabridge.db?mode=rwc"
      - "RUST_LOG=linguabridge=info,tower_http=info"
    expose:
      # Web interface - publicly accessible
      - port: 3000
        as: 80
        to:
          - global: true
      # Admin provisioning endpoint - signed requests only
      - port: 9999
        as: 9999
        to:
          - global: true
    params:
      storage:
        data:
          mount: /data
          readOnly: false

profiles:
  compute:
    inference:
      resources:
        cpu:
          units: 4
        memory:
          size: 16Gi
        storage:
          size: 20Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                - model: rtx3080
                - model: rtx3090
                - model: rtx4080
                - model: rtx4090
                - model: a100

    voice-inference:
      resources:
        cpu:
          units: 4
        memory:
          size: 24Gi
        storage:
          size: 30Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                - model: rtx3080
                - model: rtx3090
                - model: rtx4080
                - model: rtx4090
                - model: a100

    bot:
      resources:
        cpu:
          units: 1
        memory:
          size: 512Mi
        storage:
          - name: default
            size: 1Gi
          - name: data
            size: 1Gi
            attributes:
              persistent: true
              class: beta3

  placement:
    dcloud:
      attributes:
        host: akash
      signedBy:
        anyOf:
          - akash1365ez3m5ess395e8fs2zyzgqvxhfr3vqwcpqp7
          - akash18qa2a2ltfyvkyj0ggj3hkvuj6twzyumuaru9s4
      pricing:
        inference:
          denom: uakt
          amount: 10000
        voice-inference:
          denom: uakt
          amount: 12000
        bot:
          denom: uakt
          amount: 1000

deployment:
  inference:
    dcloud:
      profile: inference
      count: 1
  voice-inference:
    dcloud:
      profile: voice-inference
      count: 1
  bot:
    dcloud:
      profile: bot
      count: 1